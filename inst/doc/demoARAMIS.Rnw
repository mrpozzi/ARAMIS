%\VignetteIndexEntry{demoARAMIS}
%\VignettePackage{ARAMIS}

%\VignetteDepends{ARAMIS}
%\VignetteDepends{LearnBayes}
%\VignetteDepends{MASS}
%\VignetteDepends{mclust}

%\VignetteDepends{copula}
%\VignetteDepends{evd}

%\newcounter{enumi_saved}
%\setcounter{enumi_saved}{\value{enumi}}
%\setcounter{enumi}{\value{enumi_saved}}


\documentclass[letterpaper,pdf,english]{article}


<<results=hide,echo=FALSE>>=
options(width = 80)
set.seed(17)
@ 

\SweaveOpts{prefix.string=plotsAMIS,eps=FALSE,echo=TRUE}


\usepackage{times}
\usepackage{hyperref}
\usepackage{color}
\usepackage{babel}
\usepackage{graphicx}



\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{lscape}

\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rpackage}[1]{{\textit{#1}}}
\newcommand{\Rmethod}[1]{{\texttt{#1}}}
\newcommand{\Rfuncarg}[1]{{\texttt{#1}}}
\newcommand{\Rclass}[1]{{\textit{#1}}}
\newcommand{\Rcode}[1]{{\texttt{#1}}}
\newcommand{\software}[1]{\textsf{#1}}
\newcommand{\R}{\software{R}}

\newtheorem{theorem}{Theorem}
\newtheorem{algorithm}{Algorithm}


\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\bxx}{{\mathbf {x}}}


\usepackage[margin=1in]{geometry}
%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\rhead{}
%\renewcommand{\footrulewidth}{\headrulewidth}

\title{A R Adaptive Multiple Importance Sampling (ARAMIS)}
\author{Luca Pozzi, University of California, Berkeley, U.S.A.\\Antonietta Mira, University of Lugano, Lugano, Switzerland}
%\institute{University of Insubria, Varese, Italy}
%\url{http://www.ceremade.dauphine.fr/~xian}}


\date{Modified: November 9th, 2012, Compiled: \today}


\begin{document}
\maketitle


\begin{abstract}

ARAMIS is an R package that runs the AMIS  \cite{amis2009}
algorithm. The main features of ARAMIS are parallelization and customization.\\
ARAMIS exploits the massively parallel structure of AMIS to improve the performance of the algorithm as it was implemented in the original paper.
As a result simulation time is reduced by orders of magnitudes.

As for customization, the potential of the R language is fully exploited by ARAMIS which allows the user to taylor the software to the model which results from his or her own research setting. Target and proposal kernel can be easily specified by the user. Some working examples contained in the manual explain how this can be efficiently and easily done.

As a consequence of the flexibility and efficiency of the package, even fairly complicated problems can be accommodated, e.g. sampling from an Extreme Value (EV) Copula distribution with a mixture of EV distributions as the proposal kernel.
The latter is an interesting and useful example of how the user can specify some ``real-world" combination of target/proposal and it is added, in  the manual, to the two working examples detailed in  \cite{amis2009}.

\end{abstract}


\section{Introduction}

ARAMIS is an R package that runs AMIS  \cite{amis2009},
an Adaptive Multiple Importance Sampler that uses a set of particles that evolve over an artificial
time dimension. 
The particles are generated from an importance distribution and, properly weighted, provide a representation of
(a sample from) the target distribution, as typically done in Importance Sampling (IS).
What distinguishes  AMIS from the other IS algorithms is that old (i.e. generated at earlier times) particles are exploited to adaptively design the importance distribution in the same spirit of Adaptive Markov Chain Monte Carlo (AMCMC) where the proposal distribution  is ``adapted" instead.
The main difference from classic AMCMC, is that in this class time is a natural parameter (i.e. the evolution time of the stochastic process generated by the Metropolis-Hastings algorithm) and to each sample (that form the simulated path of the ``Markov" chain), there is no associated importance weight.
We can thus define AMIS as an hybrid between AMCMC and IS: exploiting the best from both strategies resulting in an efficient sampler with good performance also in high dimensions as proved in \cite{amis2009}.

The main features of ARAMIS are parallelization and customization.
\begin{description}
\item[Parallel implementation:] by its nature AMIS shows a embarrassingly parallel structure which is exploited by
ARAMIS to improve the performance of the algorithm and reduce simulation time by orders of magnitudes
relative to its  original implementation in \cite{amis2009}.
Following a Map-Reduce scheme the computations  are sent to the independent computing units and then the results are collected and merged together.
Each core takes care of an approximately equal set of particles evaluating the importance weights and merging them together to compute the new IS estimates at each iteration of the algorithm.
Thus, the reduction in computational tim 
is directly proportional to the number of particles used in the underlying  Importance Sampler.\\
The musketeers slogan ``all for one andone for all" can be re-interpreted in light of the parallel
features or ARAMIS that make it an hard to beat software to sample from complicated and high
dimensional target distributions.

\item[Customization:] the potential of the R language is fully exploited by ARAMIS which allows the user to taylor the software to the model which results from his or her own research setting. Target and proposal kernel can be easily specified by the user. 
\end{description}

As a consequence of the flexibility of the package, even fairly complicated problems can be accommodated, e.g. sampling from a Copula distribution with a mixture of extreme value distribution as the proposal kernel.
The latter is an interesting and useful example of how the user can specify some ``real-world" combination of target/proposal and it is added, in  the manual, to the two working examples detailed in  \cite{amis2009}.
Very little ad-hoc code is needed to sample an extreme value copula distribution: package ``copula" provides a way to evaluate the target distribution and package ``evd" offers useful tools for extreme value distributions which are easily plugged in the ARAMIS function. This is explained extensively in Section \ref{araEX}




\section{Adaptive Multiple Importance Sampling (AMIS)}

Given a density $\pi$ known up to a normalizing constant, 
and a function $h$, interest is in computing
$$
\Pi(h)=\int h(x)\pi(x)\mu(dx)
=\frac{\int h(x)\tilde\pi(x)\mu(dx)}{\int\tilde\pi(x)\mu(dx)}
$$
when $\int h(x)\tilde\pi(x)\mu(dx)$ is intractable. 
As an alternative to Metropolis-Hastings type algorithms, to estimate $\Pi(h)$ one can rely on
Importance Sampling  strategies.
An iid sample $x_1,\ldots,x_N \sim Q$ is generated from
 an importance distribution $Q$, s.t. $Q(dx)=q(x)\mu(dx)$ and  $\Pi(h)$ is estimated  by
$$
\hat\Pi^{IS}_{Q,N}(h)=N^{-1}\sum_{i=1}^Nh(x_i)
\{\pi/q\}(x_i)
$$
The IS estimator relies on alternative representation of $\Pi(h)$:
$$
\Pi(h)=\int h(x){{\mathbf {{\{\pi/q\}(x)}}q(x)}}\mu(dx)
$$


IS can be generalized to encompass much more adaptive/local schemes than thought previously. Adaptivity means learning from experience, i.e., designing new IS distributions based on the performances of earlier ones.
The intuition of the AMIS algorithm is, indeed, to use previous sample(s) to learn about $\pi$ and construct cleaver importance distributions $q$ in the same spirit as adaptive MCMC where cleaver proposal
distributions are designed based on the past history of simulated stochastic process.

Following is a description of the AMIS algorithm as it appears in  \cite{amis2009}.
The algorithm starts with an initialization phase whose rationale is to provide a good  scaling for the initial importance distribution, $Q_0$.
\begin{description}
\item[Step 0 - Initialization]: ($N_0$ particles, $t=0$) \\
\begin{itemize}
\item generate a single set of $N_0$ iid uniforms in  $\mathcal{U}_{[0,1]}^{\otimes p}$;
\item with scaled logistic transformation go back to $\Re^p$;
\item compute IS weights as function of the scale, $\tau$, of the logistic distribution;
\item use ESS(w) to find a ``good''  proposal initial scaling (as an alternative, use ESS + tempering);
\end{itemize}
\end{description}
This initial step is computationally very cheap and automatic.
\begin{description}

\item[Step 1 - Adaptation]: ($N_1$ particles, $t=1 \cdots T_1$)\\
The rationale of this step is global adaptation of the parameters of the importance distribution.
This can be achieved either by plain adaptation (Step 1) or by combining adaptation with a
 variance reduction technique  introduced  to prevent IS weights impoverishment
 by \cite{OZ00}
(Step $1^*$).
\end{description}

A potential problem when implementing Step 1 is high variability of the importance weights.
This can be solved implementing, instead, Step $1^*$.
\begin{description}
\item[Step $1^*$ - Adaptation plus Variance reduction.] The rationale of this step is twofold:
Ensure that all particles are on the same ``weighting
scale'' and can thus be easily and efficiently combined to get final
estimator;
Ensure variance reduction thanks to weights stabilization.
\end{description}


\begin{description}
\item[Step 2 - Clustering]: ($N_2$ particles, $t = T_1+1 \cdots T_2$)\\
At iteration $t = T_1+1, \cdots, T_2$, for $1\leq i\leq N_2$:
\begin{enumerate}
\item Run {\bf Rao--Blackwellised clustering algorithm}
based on (possibly trimmed by resample) past particles using
a mixture of Gaussian distributions to cluster.
The number of components, $G$, is chosen by BIC.
\item Estimate the mean, $\hat\mu_g$, and the covariance $\hat\Sigma_g$, on
each cluster $g$.
\item Simulate new sample $x_1^{t+1},\ldots,x^{t+1}_n$ from
$p$-variate mixture ($G$ components) of
$\mathcal{T}_{(3)}(\hat\mu_g,\hat\Sigma_g)$.

\item For $i=1,\ldots,N$ compute weights $\omega_i^{T+1}$ of particle
$x_i^{T+1}$ using {\bf Deterministic Mixture} idea.

\item For $1\leq l\leq T$, actualize past importance weights.
\end{enumerate}
\end{description}

The final AMIS estimator recycles all  
$
[N_0] + [N_1 \times T_1] + [N_2 \times (T_2-T_1)]
$ generated particles with the corresponding importance weights.
This is done to avoid waste of particles and to 
combine {\bf global} (step 1) and {\bf local} adaptation (step 2).


The resulting AMIS algorithm is user friendly, provides an unbiased estimator of
integrals with respect to the target and is highly efficient
requiring no tuning, allowing for automatic 
 local and global adaptation of the importance distribution. The algorithm is
 multi-purpose, interruptible and requires no
 burn-in (unlike AMCMC) and, as simulation proceeds, updates
both the weights  and the parameters of the mixture importance distribution.

\section{\Rpackage{ARAMIS} Basics: Inputs,  Outputs and Help of ARAMIS}

The structure of AMIS naturally leads to require the following inputs (to be specified by the user), for ARAMIS:

\begin{description}
\item[Inputs] (to be specified by the user):
\begin{itemize}
\item the size of the population both in the initialization, global and local adaptation phase
\item the length of the simulation in the global and local adaptation phase
\item the target distribution
\item the kernel used in the importance distribution (whose parameters are adapted as the simulation proceeds): the default kernel is a non-central T-distribution with 3 degrees of freedom (as suggested in \cite{amis2009}).
\end{itemize}
\item[Outputs] Upon running ARAMIS the following output is obtained: 
\begin{itemize}
\item generated samples and corresponding importance weights
\item effective sample size (a measure of the algorithm performance introduced in Section 4.1 of \cite{liuchen1995}))
\item mean and variance covariance matrix of the target distribution estimated by importance sampling
\end{itemize}
\item[Help] an on-line help is available with three examples worked out in details to guide the user: two are taken form the original paper  \cite{amis2009}) (a banana-shaped and a mixture distribution target), while the last one is an original example
where the target is a multivariate Extreme Value (EV) distribution obtained by a Copula construction.
\end{description}


\section{First Steps with \Rpackage{ARAMIS}}

Let's start by loading the package
<<init>>=
library(ARAMIS)
@ 
As a first example let's run an Adaptive Importance Sampling (i.e. the scheme in which the weights don't get actualized), on a ``Banana"-shaped target distribution which is defined as follows:
\begin{itemize}
\item start with a $p$-dimensional Gaussian 
$$
(x_1, ... , x_{p}) \sim N_{p}(\mathbf{0},\; {diag}(100,1,..., 1))
$$

\item change the second coordinate to

$$
y_2 = x_2 + b x_1^2-100 b
$$

\end{itemize}

\noindent for some ``Banana"-ness coeficient $b$ which is set by default to $b=0.03$.
<<AIS>>=
ais <- AIS(N=1000,
                niter=10,
                p=2,
                target= targetBanana(),
                initialize=uniInit(),
                mixture= mclustMix())
@ 

<<ais-plot, fig=TRUE, eps=FALSE,echo=FALSE>>=
par(mfrow=c(1,2))
plot(ais,whichOne=1L:2L)
@

Let's now try the AMIS algorithm on a even stronger shaped Banana target ($b=0.1$, which implies a much harder target)
<<AMIS>>=
amis <- AMIS(N=1000,
                      niter=10,
                      p=2,
                      target=targetBanana(b=0.1),
                      initialize= amisInit(maxit=5000),
                       mixture=mclustMix())
@

<<amis-plot, fig=TRUE, eps=FALSE,echo=FALSE>>=
par(mfrow=c(1,2))
plot(amis,whichOne=1L:2L,n=300)
@

The \Robject{ISO} object returned by the function contains the sample from the target distribution and some further useful information.

<<show>>=
showClass("ISO")
summary(amis)
@

\Robject{ISO} objects allow the use of familiar syntax
<<exampleOfSubsetting>>=
amis$ESS
ais[["ESS"]]
@ 
but have also other useful methods, like \Rfunction{plot}, \Rfunction{mean} and \Rfunction{var}: let's use them for a simple sanity check:
<<meanVar,echo=FALSE>>=
sqr <- function(x){x^2} 
mean(amis,fun=sqr) - mean(amis)^2
var(amis)
@

%\Rfunction{attach}  method is also available to access \Robject{ISO} slot \Robject{envir} and to \Rfunction{attach} it making available two functions to compute the target's CDF and to simulate from the target distribution
%<<attach>>=
%attach(amis)
%rBanana(1)
%pBanana(c(0,20))
%@


\section{How to use \Rpackage{ARAMIS} for your Research}\label{araEX}


The purpose of this section is to illustrate the functionality of the package on a real world example and to briefly show how the algorithm can be personalized to respond to your needs.

The algorithm is written in a general form so that the user can specify his favorite combination of target distribution, component of the proposal distribution and clustering algorithm, the way to really exploit all the flexibility is to use the technique called function closure.

To illustrate the use of function closures let's try the algorithm on a hard-to-tackle-problem. Let's use as a target the following Galambos copula

<<seed,results=hide,echo=FALSE>>=
set.seed(1264649575)
@

<<galambos>>=
# Galambos Copula
# a : location parameter GEV
# b : scale parameter GEV
# s : shape parameter GEV
# theta : copula's parameter

galambosCop <- function(a=0,b=1,s=0,theta=1.5,copula=galambosCopula){

	require("copula")
	require("evd")
	
	Cop <- copula(theta)
	
	function(xx){
		
		U <- pgev(xx,loc=a, scale=b,shape=s) 
		# numerical stability can be a issue...
		U[U>1-sqrt(.Machine$double.eps)] <- 1-sqrt(.Machine$double.eps)
		U[U<sqrt(.Machine$double.eps)] <- sqrt(.Machine$double.eps)
		
		if(length(dim(xx)>1)){
			trgt <- log(dCopula(U,Cop))+
			           rowSums(apply(xx,2,function(x)
			                      dgev(x,loc=a, scale=b,shape = s,log=TRUE)))
			}else{
				trgt <- log(dCopula(U,Cop))+
				             sum(dgev(xx,loc=a, scale=b,shape=s,log=TRUE))
				}
		
		trgt
		
		}	
	}
@



This is what the function closure consists into: to build an environment around the function that contains all the parameters the function needs but that are not being passed as an argument. This allows us to make the list of arguments completely general, in the example \Rfuncarg{a}, \Rfuncarg{b}, \Rfuncarg{s}, etc... are enclosed in scope of the function

<<galambosClose>>=
toughCop <- galambosCop()
ls(envir=environment(toughCop))
get("theta",envir=environment(toughCop))
@

We can now run \Rfunction{AMIS} and see from the plot how this is fully captured by the algorithm.


<<galambosAMIS, fig=TRUE, eps=FALSE, height=5>>=

resCop <- AMIS(N=c(500,1000,500),
                           niter=c(10,50),
                           p=2,
                          target=toughCop,
                           initialize= amisInit(maxit=5000),
                           mixture=mclustMix())


plot(resCop,whichOne=3L, xlim=c(-3,5),ylim=c(-3,5),N=17000)

@


\section{Parallelization}

The evaluation of a particle is at each iteration  completely independent from the others. \Rfunction{ARAMIS} exploits this feature to speed up computations considerably by equally sharing the burden of simulating the particles and computing the weights between the cores, then collecting the result and computing the summary statistics of the Importance Sample, in a Map-Reduce framework.

In practice the load gets evenly divided between the available cores and then the results merged for computing the mean and variance, for each iteration.

<<parallel, eps=FALSE, fig=TRUE, height=5>>=
trgt <- targetBanana(b=0.1)
mxtr <- mclustMix()
system.time(amis <- AMIS(N=1000,niter=10,p=2,target=trgt,mixture=mxtr))	
invisible(gc())
system.time(amisMC <- AMIS(N=1000,niter=10,p=2,target=trgt,mixture=mxtr,parallel="multicore",nCores=2))	
invisible(gc())
system.time(amisSN <- AMIS(N=1000,niter=10,p=2,target=trgt,mixture=mxtr,parallel="snow",nCores=2))


summary(amis)
summary(amisMC)
summary(amisSN)
	
par(mfrow=c(1,3))
plot(amis,whichOne=1L)
plot(amisMC,whichOne=1L)
plot(amisSN,whichOne=1L)

@

\section*{Acknowledgments} 

We thank J.M.~Marin for providing the code to run the AMIS algorithm on the examples of the paper \cite{amis2009} and M.~Garieri for helping with many computational issues and frustrations.

\begin{thebibliography}{30}


\bibitem{amis2009}
Cornuet, J.M. and Marin, J.M. and Mira, A. and Robert,C. \emph{Adaptive Multiple Importance Sampling}. Scandinavian Journal of Statistics (2012).

\bibitem{liuchen1995} Liu, J. and Chen, R. \emph{Blind Deconvolution via Sequential Imputations}. Journal of the American Statistical Association (1995).
% ma forse  meglio citare il libro di Jun Liu invece di questo articolo ...

\bibitem{OZ00}
Owen, A. and Zhou, Y. \emph{Safe and Effective Importance Sampling}. Journal of the American Statistical Association (2000).

\bibitem{RobRos2009}
Gareth, O. and Roberts,C. and. Rosenthal, J.S.. \emph{Examples of Adaptive MCMC}. Journal of Computational and Graphical Statistics (2009).

\bibitem{TierMir1999}
Tierney L. and Mira A. \emph{Some Adaptive Monte Carlo Methods for Bayesian Inference}. Statistics in Medicine (1999).


\end{thebibliography}



\section*{SessionInfo}

<<sessionInfo,results=tex,echo=FALSE>>=
toLatex(sessionInfo())
@ 

\end{document}

